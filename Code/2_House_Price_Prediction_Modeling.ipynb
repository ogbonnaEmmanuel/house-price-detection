{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('df1.pickle','rb') as read_file:\n",
    "    df1 = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge #ordinary linear regression + w/ ridge regularization\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data to training/validation and testing:\n",
    "\n",
    "Splitting to training/testing accurately measuring how well our model generalizes to new data. \n",
    "\n",
    "Further split training to to 10-fold training/validation is used to select the model that performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: If we do not remove missing, it will show error message \"Input contains NaN, infinity or a value too large for dtype('float64')\". So removing missing is a must before training model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 10 fold cross validation\n",
    "y = df1['total_price_log']\n",
    "# X = df1[['parking_scaled','# of beds_scaled','# of baths_scaled','sqft_scaled','Type_Apartment','Type_Condo','Type_Multi Family','Type_Single Family','City_SAN FRANCISCO','City_SAN MATEO','City_DALY CITY','City_PACIFICA']]\n",
    "X = df1[['parking','# of beds','# of baths','sqft','Type_Apartment','Type_Condo','Type_Multi Family','Type_Single Family','City_SAN FRANCISCO','City_SAN MATEO','City_DALY CITY','City_PACIFICA']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# hold out 20% of the data for final testing\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "#this helps with the way kf will generate indices below\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "#run the CV\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state = 42)\n",
    "cv_lm_r2s, cv_lm_reg_r2s, cv_lm_poly_r2s, cv_huber_r2s  = [], [], [], [] #collect the validation results for both models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between different error metrics:\n",
    "\n",
    "**1. Mean_squared_error (MSE):** It's the average of squared difference between prediction and actual observation\n",
    "\n",
    "**2. Mean_absolute_error (MAE):** Measures the average magnitude of the errors in a set of predictions, without considering their direction. All individual difference have equal weight\n",
    "\n",
    "**3. Root_mean_squared_error (RMSE):** It's the square root of the average of squared differences between prediction and actual observation\n",
    "\n",
    "*Difference between MAE and MSE:*\n",
    "\n",
    "a. Because MSE square the differene before they are averaged, the MSE gives a relatively high weight to large errors. **This means the RMSE should be more useful when large errors are particularly undesirable.** (MAE is more robust to outliers, MSE is more useful if we are concerned about larger errors)\n",
    "b. MAE is steady and RMSE increases as the variance associated with the frequency distribution of error magnitudes also increases\n",
    "c. RMSE has a tendency to be increasingly larger than MAE as the test sample size increases\n",
    "d. From an interpretation standpoint, MAE is clearly the winner. RMSE does not describe average error alone and has other implications that are more difficult to tease out and understand\n",
    "e. On the other hand, one distinct advantage of RMSE over MAE is that RMSE avoids the use of taking the absolute value, which is undesirable in many mathematic calculations. MSE has nice mathematical properties which makes it easier to compute using gradient descent. MAE requires more complicated tools such as linear programming to compute the gradient.\n",
    "\n",
    "*Similarity between MAE and MSE:*\n",
    "\n",
    "a. Both express average model prediction error\n",
    "b. Both range from 0 to infinite(+) and are indifferent to the direction of errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply linear regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple regression scores:  [0.20667816856133708, 0.21936239455427214, 0.24352477407051412, 0.20326828378264827, 0.19303707021300082, 0.24865290698519293, 0.2570076158258979, 0.23355328937833633, 0.23844428306001184, 0.26636925522395705] \n",
      "\n",
      "Simple mean cv mean_squared_error: 0.231 +- 0.023\n"
     ]
    }
   ],
   "source": [
    "for train_ind, val_ind in kf.split(X,y):\n",
    "    \n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind] \n",
    "    \n",
    "    # simple linear regression\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train, y_train)\n",
    "    # applying with or without standardized features to the linear regression does not make much difference !!???\n",
    "    cv_lm_r2s.append(np.sqrt(mean_squared_error(y_val, lm.predict(X_val))))\n",
    "    \n",
    "print('Simple regression scores: ', cv_lm_r2s,'\\n')\n",
    "\n",
    "print(f'Simple mean cv mean_squared_error: {np.mean(cv_lm_r2s):.3f} +- {np.std(cv_lm_r2s):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Ridge/Lasso regularization\n",
    "\n",
    "In Regularized Linear Regression, there is an additional component of the cost function that penalizes the \"size\" of the coefficients.\n",
    "\n",
    "Why penalize a coefficient? At the simplest level, it forces a variable to be \"worth it\" in order to have a particularly extreme coefficient or even one that's greater than zero. The intuition is that it is a \"simpler model\" to have smaller coefficients (in absolute value) than larger ones - regularization means that the coefficients are constrained to lie within a narrower region, making model coefficients more stable and less extreme.\n",
    "\n",
    "**When using regularization, we must standardize** the data so that all features are on the same scale (we subtract the mean of each column and divide by the standard deviation, giving us features with mean 0 and std 1). Since this scaling is part of our model, we need to scale using the training set feature distributions and apply the same scaling to validation and test without refitting the scaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO**:\n",
    "* _Pro_: great for trimming features and focusing interpretation on a few key ones\n",
    "* _Con_: risk of discarding features that are actually useful\n",
    "\n",
    "**Ridge**:\n",
    "* _Pro_: great for smoothly handling multicollinearity (in case when multicollinearity exists, although beta_hat is unbiased estimator(under OLS), the variance of beta_hat is very large. Ridge works well on stablize variance of beta_hat), very nice when working with sparse features \n",
    "* _Con_: will never fully discard (collinear) features\n",
    "\n",
    "If the mapping from features to target truly depends on only a few key features, LASSO should outperform. If instead the target actually depends on many features (even if only a little dependent), Ridge should work better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning regularization strength\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "alphalist = 10**(np.linspace(-2,2,200))\n",
    "err_vec_val = np.zeros(len(alphalist))\n",
    "err_vec_train = np.zeros(len(alphalist))\n",
    "\n",
    "#Mean Absolute Error (MAE)\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_pred - y_true)) \n",
    "\n",
    "for i,curr_alpha in enumerate(alphalist):\n",
    "\n",
    "    # note the use of a new sklearn utility: Pipeline to pack\n",
    "    # multiple modeling steps into one fitting process \n",
    "    steps = [('standardize', StandardScaler()), \n",
    "             ('lasso', Lasso(alpha = curr_alpha))]\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    val_set_pred = pipe.predict(X_val)\n",
    "    err_vec_val[i] = mae(y_val, val_set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the curve of validation error as alpha changes\n",
    "\n",
    "# plt.plot(np.log10(alphalist), err_vec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the value of alpha that gave us the lowest error\n",
    "alphalist[np.argmin(err_vec_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso scores:  [0.2069487081072591, 0.2194164015513407, 0.24309814140551478, 0.20312255562087939, 0.19261595392211753, 0.2482366180563881, 0.25666206625186294, 0.23356558380867815, 0.23837985996902597, 0.2664925645093741] \n",
      "\n",
      "Lasso mean mean_squared_error: 0.231 +- 0.023\n"
     ]
    }
   ],
   "source": [
    "cv_lasso_r2s = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X,y):\n",
    "    \n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind] \n",
    "    \n",
    "    lasso_model = Lasso(alpha = 0.001) # this is a VERY HIGH regularization strength!, wouldn't usually be used\n",
    "\n",
    "    #ridge with feature scaling, which standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    cv_lasso_r2s.append(np.sqrt(mean_squared_error(y_val, lasso_model.predict(X_val_scaled))))\n",
    "    \n",
    "print('Lasso scores: ', cv_lasso_r2s, '\\n')\n",
    "\n",
    "print(f'Lasso mean mean_squared_error: {np.mean(cv_lasso_r2s):.3f} +- {np.std(cv_lasso_r2s):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_predict_test = lasso_model.predict(X_test_scaled) \n",
    "# lasso_predict_all = lasso_model.predict(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge scores:  [0.2066781738027288, 0.21936242069832546, 0.2435247611868896, 0.20326829729272014, 0.19303701835813672, 0.24865285177135282, 0.2570076273990186, 0.23355330156625706, 0.23844428968196812, 0.2663692766084271] \n",
      "\n",
      "Ridge mean mean_squared_error: 0.231 +- 0.023\n"
     ]
    }
   ],
   "source": [
    "for train_ind, val_ind in kf.split(X,y):\n",
    "    \n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind] \n",
    "    \n",
    "    lm_reg = Ridge(alpha=0.001)\n",
    "\n",
    "    #ridge with feature scaling, which standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    lm_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    cv_lm_reg_r2s.append(np.sqrt(mean_squared_error(y_val, lm_reg.predict(X_val_scaled))))\n",
    "    \n",
    "print('Ridge scores: ', cv_lm_reg_r2s, '\\n')\n",
    "\n",
    "print(f'Ridge mean mean_squared_error: {np.mean(cv_lm_reg_r2s):.3f} +- {np.std(cv_lm_reg_r2s):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Huber Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber Loss Function takes the best properties of the L1 and L2 loss by being convex close to the target and steep for extreme values, usually are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple regression scores:  [0.2041128394847529, 0.22214770117492688, 0.24616682748367688, 0.20655030608436648, 0.19421253880622802, 0.25037617534160816, 0.25908143006856615, 0.23336938853743175, 0.2383819642399312, 0.26701908530573887] \n",
      "\n",
      "Simple mean mean_squared_error: 0.232 +- 0.023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import HuberRegressor, LinearRegression\n",
    "\n",
    "for train_ind, val_ind in kf.split(X,y):\n",
    "    \n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind] \n",
    "    \n",
    "    #ridge with feature scaling, which standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "   \n",
    "    huber = HuberRegressor().fit(X_train_scaled, y_train)\n",
    "    # huber.score(X, y) \n",
    "\n",
    "    cv_huber_r2s.append(np.sqrt(mean_squared_error(y_val, huber.predict(X_val_scaled))))\n",
    "    \n",
    "print('Simple regression scores: ', cv_huber_r2s,'\\n')\n",
    "\n",
    "print(f'Simple mean mean_squared_error: {np.mean(cv_huber_r2s):.3f} +- {np.std(cv_huber_r2s):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Polynomial regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poly regression scores:  [0.31024140215102775, 0.7261997995031676, 1.760338315461797, 0.3858166716222978, 1.1959924164942164, 0.28479470554920805, 0.3458231004237437, 0.7815608430366922, 1.1591506888283656, 0.37900170717572806]\n",
      "Poly mean cv r^2: 0.733 +- 0.472\n"
     ]
    }
   ],
   "source": [
    "for train_ind, val_ind in kf.split(X,y):\n",
    "    \n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind] \n",
    "    \n",
    "    poly = PolynomialFeatures(degree=3) \n",
    "\n",
    "    #polynomial regression\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    lm_poly = LinearRegression()\n",
    "    lm_poly.fit(X_train_poly, y_train)\n",
    "    cv_lm_poly_r2s.append(np.sqrt(mean_squared_error(y_val, lm_poly.predict(X_val_poly))))\n",
    "\n",
    "print('Poly regression scores: ', cv_lm_poly_r2s)\n",
    "\n",
    "print(f'Poly mean cv r^2: {np.mean(cv_lm_poly_r2s):.3f} +- {np.std(cv_lm_poly_r2s):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above shows different regression has similar MSE. The lasso model appears to be both better on average and has less varying results.\n",
    "\n",
    "Since k-fold is more reliable than a single validation set, we select the linear regression(OLS) model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
